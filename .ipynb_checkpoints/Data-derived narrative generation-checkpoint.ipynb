{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "import random\n",
    "from fiona.crs import from_epsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, json, requests, geojson\n",
    "import googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import (\n",
    "    Map, GeoData, GeoJSON, basemaps, basemap_to_tiles, \n",
    "    Icon, Circle, Marker,\n",
    "    LayerGroup, WidgetControl)\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, Layout\n",
    "from IPython.display import display, clear_output, Markdown as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output()\n",
    "data_output = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need this to stop numpy from returning truncated arrays \n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# for automatic linebreaks and multi-line cells\n",
    "pd.options.display.max_colwidth = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748d538a18b744d1855d09dd2d7f68d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[40.7210907, -73.9877836], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_titlâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#drawing basic map\n",
    "center = (40.7210907,-73.9877836)\n",
    "basemap = basemap_to_tiles(basemaps.CartoDB.Positron)\n",
    "\n",
    "m = Map(layers=(basemap, ), center=center, zoom=14, min_zoom = 7, max_zoom = 20)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location():       \n",
    "    global gdf, lat, lon\n",
    "    \n",
    "    lat = str(markerlocation[0])\n",
    "    lon = str(markerlocation[1])\n",
    "    \n",
    "    df2 = pd.DataFrame(markerlocation)\n",
    "    df=df2.transpose()\n",
    "    df.columns=['Latitude','Longitude']\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude), crs='epsg:4326')\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "draggable=False\n",
    "marker_opacity=0\n",
    "icon = Icon(icon_url='icon.png', icon_size=[15, 15])\n",
    "\n",
    "marker = Marker(location=center, draggable=draggable, icon=icon, opacity=marker_opacity)\n",
    "studyarea = Circle(location=center, radius=420, color=\"black\", fill_color=\"white\", fill_opacity = .2, weight=2)\n",
    "\n",
    "markerlocation = marker.location \n",
    "    \n",
    "layer_group = LayerGroup(layers=(marker, studyarea))\n",
    "m.add_layer(layer_group)\n",
    "   \n",
    "def update_marker(**kwargs):\n",
    "    \n",
    "    if kwargs.get('type') == 'click':\n",
    "        layer_group.clear_layers();\n",
    "        \n",
    "        global studyarea\n",
    "        \n",
    "        marker = Marker(location=kwargs.get('coordinates'), draggable=draggable, icon=icon, opacity=marker_opacity, options=['rise_on_hover'])  \n",
    "        studyarea = Circle(location=kwargs.get('coordinates'), radius=420, color=\"black\", fill_color=\"white\", fill_opacity = .2, weight=2)\n",
    "\n",
    "        global markerlocation\n",
    "        markerlocation = marker.location \n",
    "        \n",
    "        layer_group.add_layer(marker)\n",
    "        layer_group.add_layer(studyarea)\n",
    "    \n",
    "        draw_update_buffer(**kwargs)\n",
    "        \n",
    "        output.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_update_buffer(**kwargs):     \n",
    "    m.on_interaction(update_marker)\n",
    "    extract_location()\n",
    "        \n",
    "    global half_mi\n",
    "    half_mi=gdf.copy()\n",
    "    half_mi['geometry'] = half_mi.geometry.buffer(.004,  cap_style=1, join_style=1)\n",
    "\n",
    "draw_update_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_censustracts():\n",
    "    extract_location()\n",
    "    \n",
    "    bounding_box = half_mi.envelope\n",
    "    df = gpd.GeoDataFrame(gpd.GeoSeries(bounding_box), columns=['geometry'])\n",
    "    minx, miny, maxx, maxy = df.geometry.total_bounds\n",
    "    bounds = minx, miny, maxx, maxy\n",
    "\n",
    "    # census tracts link\n",
    "    endpoint = 'https://tigerweb.geo.census.gov/arcgis/rest/services/TIGERweb/Tracts_Blocks/MapServer/4/query'\n",
    "    s = requests.session()\n",
    "    s.params = {\n",
    "        'geometry': str(bounds),\n",
    "        'geometryType': 'esriGeometryEnvelope',\n",
    "        'inSR': 4326,\n",
    "        'spatialRel': 'esriSpatialRelIntersects',\n",
    "        'outFields': 'GEOID,STATE,COUNTY,TRACT,NAME,STGEOMETRY,OBJECTID',\n",
    "        'returnGeometry': True,\n",
    "        'f': 'geojson',        \n",
    "    }\n",
    "    start = 0\n",
    "    done = False\n",
    "    features = []\n",
    "    crs = None\n",
    "    while not done:\n",
    "        r = s.get(endpoint, params={\n",
    "            'resultOffset': start,\n",
    "            'resultRecordCount': 32,\n",
    "        })\n",
    "        censusgeo = geojson.loads(r.text)\n",
    "        newfeats = censusgeo.__geo_interface__['features']\n",
    "        if newfeats:\n",
    "            features.extend(newfeats)\n",
    "            crs=censusgeo.__geo_interface__['crs']\n",
    "            start += len(newfeats)\n",
    "        else:\n",
    "            done = True\n",
    "    \n",
    "    global tracts\n",
    "    tracts = gpd.GeoDataFrame.from_features(features, crs=crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_acs():  \n",
    "    state = tracts[\"STATE\"].unique().tolist()\n",
    "    state = ', '.join(map(str, state)).replace(\" \", \"\")\n",
    "\n",
    "    tract = tracts[\"TRACT\"].unique().tolist()\n",
    "    tract = ', '.join(map(str, tract)).replace(\" \", \"\") \n",
    "\n",
    "    county = tracts[\"COUNTY\"].unique().tolist()\n",
    "    county = ', '.join(map(str, county)).replace(\" \", \"\") \n",
    "\n",
    "    api_key = '9330dc4bf086a84f19fb412bb15f232507301de6'\n",
    "    acs_url = f'https://api.census.gov/data/2018/acs/acs5/subject/'\n",
    "    \n",
    "    global acs_variables\n",
    "    acs_variables_initial = 'S1603_C02_002E,S1603_C02_003E,S1603_C02_004E,S1603_C04_002E,S1603_C04_003E,S1603_C04_004E,S1601_C01_005E,S1601_C01_006E,S1601_C01_007E,S1601_C01_009E,S1601_C01_010E,S1601_C01_011E,S1601_C01_013E,S1601_C01_014E,S1601_C01_015E,S1601_C01_017E,S1601_C01_018E,S1601_C01_019E,S1901_C01_002E,S1901_C01_003E,S1901_C01_004E,S1901_C01_005E,S1901_C01_006E,S1901_C01_007E,S1901_C01_008E,S1901_C01_009E,S1901_C01_010E,S1901_C01_011E,S1901_C04_002E,S1901_C04_003E,S1901_C04_004E,S1901_C04_005E,S1901_C04_006E,S1901_C04_007E,S1901_C04_008E,S1901_C04_009E,S1901_C04_010E,S1901_C04_011E'\n",
    "    acs_variables_additional = 'S1501_C01_002E,S1501_C01_004E,S1501_C01_003E,S1501_C01_005E,S1501_C01_017E,S1501_C01_018E,S1501_C01_020E,S1501_C01_021E,S1501_C01_023E,S1501_C01_024E,S1501_C01_025E,S1501_C01_026E,S1501_C03_002E,S1501_C03_003E,S1501_C03_004E,S1501_C03_005E,S1501_C03_017E,S1501_C03_018E,S1501_C03_020E,S1501_C03_021E,S1501_C03_023E,S1501_C03_024E,S1501_C03_026E,S1501_C03_027E,S1501_C05_002E,S1501_C05_003E,S1501_C05_004E,S1501_C05_005E,S1501_C05_017E,S1501_C05_018E,S1501_C05_020E,S1501_C05_021E,S1501_C05_023E,S1501_C05_024E,S1501_C05_026E,S1501_C05_027E,S1401_C01_030E,S1401_C01_032E,S1401_C01_034E,S1101_C01_003E,S1101_C05_001E'\n",
    "    acs_variables = acs_variables_initial + \",\" + acs_variables_additional\n",
    "    \n",
    "    get_acs_initial = f'{acs_url}?&get={acs_variables_initial}&for=tract:{tract}&in=state:{state}%20county:{county}&key={api_key}'\n",
    "    get_acs_additional = f'{acs_url}?&get={acs_variables_additional}&for=tract:{tract}&in=state:{state}%20county:{county}&key={api_key}'\n",
    "\n",
    "    data_acs_initial=requests.get(get_acs_initial).json()\n",
    "    data_acs_additional=requests.get(get_acs_additional).json()\n",
    "    \n",
    "    global acs\n",
    "    acs_initial=pd.DataFrame(data_acs_initial[1:], columns=data_acs_initial[0])\n",
    "    acs_additional=pd.DataFrame(data_acs_additional[1:], columns=data_acs_additional[0])\n",
    "\n",
    "    acs=pd.merge(acs_initial, acs_additional, on='tract', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_combine_census_and_geographic_data():\n",
    "    import_censustracts()\n",
    "    download_acs()\n",
    "    \n",
    "    global acs_site_sum, acs_site\n",
    "    tracts[\"area\"]=tracts.area\n",
    "    acs_tracts = pd.merge(tracts, acs, left_on='TRACT', right_on='tract', how='left')\n",
    "    \n",
    "    acs_site = gpd.overlay(half_mi, acs_tracts, how='intersection')\n",
    "    acs_site[\"area_clipped\"]=acs_site.area \n",
    "    acs_site[\"ratio\"] = acs_site[\"area_clipped\"]/acs_site[\"area\"]\n",
    "    \n",
    "    cols = acs_variables.split(\",\")\n",
    "    acs_site[cols] = acs_site[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "    \n",
    "    temp_df = acs_site[cols]    \n",
    "    temp_df = temp_df.mul(acs_site.ratio, 0)\n",
    "    acs_site.update(temp_df)\n",
    "\n",
    "    acs_site_sum = pd.DataFrame(acs_site[cols].sum())\n",
    "\n",
    "    acs_site_sum.reset_index(inplace=True)\n",
    "    acs_site_sum.columns = ['variables', 'sum_in_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acs_site.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>variable_group</th>\n",
       "      <th>variables</th>\n",
       "      <th>variable_name</th>\n",
       "      <th>age_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Male Female</td>\n",
       "      <td>5 to 17 years</td>\n",
       "      <td>Language Spoken At Home</td>\n",
       "      <td>S1603_C02_002E</td>\n",
       "      <td>Speak Only English at Home</td>\n",
       "      <td>child, teenager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Male Female</td>\n",
       "      <td>18 to 64 years</td>\n",
       "      <td>Language Spoken At Home</td>\n",
       "      <td>S1603_C02_003E</td>\n",
       "      <td>Speak Only English at Home</td>\n",
       "      <td>young adult, middle aged adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Male Female</td>\n",
       "      <td>65 years and over</td>\n",
       "      <td>Language Spoken At Home</td>\n",
       "      <td>S1603_C02_004E</td>\n",
       "      <td>Speak Only English at Home</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Male Female</td>\n",
       "      <td>5 to 17 years</td>\n",
       "      <td>Language Spoken At Home</td>\n",
       "      <td>S1601_C01_005E</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>child, teenager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Male Female</td>\n",
       "      <td>18 to 64 years</td>\n",
       "      <td>Language Spoken At Home</td>\n",
       "      <td>S1601_C01_006E</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>young adult, middle aged adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sex           age_group           variable_group       variables  \\\n",
       "0  Male Female       5 to 17 years  Language Spoken At Home  S1603_C02_002E   \n",
       "1  Male Female      18 to 64 years  Language Spoken At Home  S1603_C02_003E   \n",
       "2  Male Female  65 years and over   Language Spoken At Home  S1603_C02_004E   \n",
       "3  Male Female       5 to 17 years  Language Spoken At Home  S1601_C01_005E   \n",
       "4  Male Female      18 to 64 years  Language Spoken At Home  S1601_C01_006E   \n",
       "\n",
       "                variable_name                    age_category  \n",
       "0  Speak Only English at Home                 child, teenager  \n",
       "1  Speak Only English at Home  young adult, middle aged adult  \n",
       "2  Speak Only English at Home                             old  \n",
       "3                     Spanish                 child, teenager  \n",
       "4                     Spanish  young adult, middle aged adult  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = pd.read_csv(\"data-dictionary.csv\")\n",
    "data_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def user_selection():\n",
    "\n",
    "    global selected_persona, selected_percentile, text_generation_button, selection_filter, variable_inputs\n",
    "    \n",
    "    selected_percentile = widgets.SelectionSlider(\n",
    "        options = ['10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'],  \n",
    "        value='50%', \n",
    "        description='Percentile:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "    )\n",
    "        \n",
    "    selected_persona = widgets.ToggleButtons(\n",
    "                        options=['Boy', 'Teenage Boy', 'Young Man', 'Middle Aged Man', 'Old Man',\n",
    "                                 'Girl', 'Teenage Girl', 'Young Woman', 'Middle Aged Woman', 'Old Woman'],\n",
    "                        description='AGE:',\n",
    "                        disabled=False,\n",
    "                        button_style='', \n",
    "                    )\n",
    "\n",
    "    selected_persona_str = str(selected_persona.value)    \n",
    "    \n",
    "    if (selected_persona.value == 'Boy'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('child')) & \\\n",
    "                              (data_dict.sex.str.contains('Male'))]\n",
    "    elif (selected_persona.value == 'Girl'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('child')) & \\\n",
    "                              (data_dict.sex.str.contains('Female'))]\n",
    "        \n",
    "    elif (selected_persona.value == 'Teenage Boy'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('teenager')) & \\\n",
    "                              (data_dict.sex.str.contains('Male'))]\n",
    "    elif (selected_persona.value == 'Teenage Girl'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('teenager')) & \\\n",
    "                              (data_dict.sex.str.contains('Female'))] \n",
    "\n",
    "    elif (selected_persona.value == 'Young Man'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('young')) & \\\n",
    "                          (data_dict.sex.str.contains('Male'))]\n",
    "    elif (selected_persona.value == 'Young Woman'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('young')) & \\\n",
    "                          (data_dict.sex.str.contains('Female'))]\n",
    "\n",
    "    elif (selected_persona.value == 'Middle Aged Man'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('middle aged')) & \\\n",
    "                          (data_dict.sex.str.contains('Male'))]\n",
    "    elif (selected_persona.value == 'Middle Aged Woman'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('middle aged')) & \\\n",
    "                          (data_dict.sex.str.contains('Female'))]\n",
    "        \n",
    "    elif (selected_persona.value == 'Old Man'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('old')) & \\\n",
    "                              (data_dict.sex.str.contains('Male'))]\n",
    "    elif (selected_persona.value == 'Old Woman'):   \n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('old')) & \\\n",
    "                              (data_dict.sex.str.contains('Female'))]\n",
    "                         \n",
    "    list_of_variable_inputs = selection_filter[\"variables\"].values[0:]\n",
    "    variable_inputs = ', '.join(list_of_variable_inputs).replace(\" \", \"\")\n",
    "    variable_inputs = variable_inputs.split(',')\n",
    "        \n",
    "user_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_filtering(age_category, sex):\n",
    "    \n",
    "    selected_persona_str = str(selected_persona.value)    \n",
    "\n",
    "    if (selected_persona.value == 'Boy'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('child')) & \\\n",
    "                              (data_dict.sex.str.contains('Male'))]\n",
    "    elif (selected_persona.value == 'Girl'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('child')) & \\\n",
    "                              (data_dict.sex.str.contains('Female'))]\n",
    "        \n",
    "    elif (selected_persona.value == 'Teenage Boy'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('teenager')) & \\\n",
    "                              (data_dict.sex.str.contains('Male'))]\n",
    "    elif (selected_persona.value == 'Teenage Girl'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('teenager')) & \\\n",
    "                              (data_dict.sex.str.contains('Female'))] \n",
    "\n",
    "    elif (selected_persona.value == 'Young Man'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('young')) & \\\n",
    "                          (data_dict.sex.str.contains('Male'))]\n",
    "    elif (selected_persona.value == 'Young Woman'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('young')) & \\\n",
    "                          (data_dict.sex.str.contains('Female'))]\n",
    "\n",
    "    elif (selected_persona.value == 'Middle Aged Man'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('middle aged')) & \\\n",
    "                          (data_dict.sex.str.contains('Male'))]\n",
    "    elif (selected_persona.value == 'Middle Aged Woman'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('middle aged')) & \\\n",
    "                          (data_dict.sex.str.contains('Female'))]\n",
    "        \n",
    "    elif (selected_persona.value == 'Old Man'):\n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('old')) & \\\n",
    "                              (data_dict.sex.str.contains('Male'))]\n",
    "    elif (selected_persona.value == 'Old Woman'):   \n",
    "        selection_filter = data_dict[(data_dict.age_category.str.contains('old')) & \\\n",
    "                              (data_dict.sex.str.contains('Female'))]\n",
    "\n",
    "    list_of_variable_inputs = selection_filter[\"variables\"].values[0:]\n",
    "    variable_inputs = ', '.join(list_of_variable_inputs).replace(\" \", \"\")\n",
    "    variable_inputs = variable_inputs.split(',')        \n",
    "    \n",
    "    data_output.clear_output()\n",
    "    output.clear_output()\n",
    "\n",
    "def selected_persona_eventhandler(change):\n",
    "    selection_filtering(change.new, selected_persona.value)\n",
    "def selected_percentile_eventhandler(change):\n",
    "    selection_filtering(selected_percentile.value, change.new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demographics_for_selection():\n",
    "  \n",
    "    global percentile_input, data\n",
    "\n",
    "    data = pd.merge(acs_site_sum.loc[acs_site_sum['variables'].isin(variable_inputs)], \\\n",
    "                   selection_filter, how=\"outer\", on=\"variables\")    \n",
    "    data[\"sum_in_area\"] = data[\"sum_in_area\"].astype(int)\n",
    "    data.sort_values(\"sum_in_area\", axis = 0, ascending = True, inplace = True)\n",
    "\n",
    "    percentile_input = int(selected_percentile.value.strip('%')) / 100\n",
    "    \n",
    "# split these up into the diff bins for different types of variable groups \n",
    "    global language, education, family_household_income, nonfamily_household_income, household_type\n",
    "\n",
    "    for item,i in enumerate(data):       \n",
    "        language = data[(data[\"variable_group\"].str.contains('Language'))]\n",
    "        education = data[(data[\"variable_group\"].str.contains('Educational Attainment'))]\n",
    "        family_household_income = data[(data[\"variable_group\"].str.contains('Family'))]\n",
    "        nonfamily_household_income = data[(data[\"variable_group\"].str.contains('Nonfamily'))]\n",
    "        household_type = data[(data[\"variable_group\"].str.contains('Households'))]\n",
    "\n",
    "#Calculate individual percentile values\n",
    "        global sum_for_percentile_language,\\\n",
    "                sum_for_percentile_education,\\\n",
    "                sum_for_percentile_family_household_income,\\\n",
    "                sum_for_percentile_nonfamily_household_income, \\\n",
    "                sum_for_percentile_household_type\n",
    "        \n",
    "        sum_for_percentile_language = language.sum_in_area.quantile(percentile_input).astype(str)\n",
    "        sum_for_percentile_language = sum_for_percentile_language.replace(sum_for_percentile_language, \\\n",
    "                            language.sum_in_area.quantile(percentile_input).astype(str))\n",
    "\n",
    "        sum_for_percentile_education = education.sum_in_area.quantile(percentile_input).astype(str)\n",
    "        sum_for_percentile_education = sum_for_percentile_education.replace(sum_for_percentile_education, \\\n",
    "                            education.sum_in_area.quantile(percentile_input).astype(str))\n",
    "\n",
    "        sum_for_percentile_family_household_income = family_household_income.sum_in_area.quantile(percentile_input).astype(str)\n",
    "        sum_for_percentile_family_household_income = sum_for_percentile_family_household_income.replace(sum_for_percentile_family_household_income, \\\n",
    "                            family_household_income.sum_in_area.quantile(percentile_input).astype(str))\n",
    "\n",
    "        sum_for_percentile_nonfamily_household_income = nonfamily_household_income.sum_in_area.quantile(percentile_input).astype(str)\n",
    "        sum_for_percentile_nonfamily_household_income = sum_for_percentile_nonfamily_household_income.replace(sum_for_percentile_nonfamily_household_income, \\\n",
    "                            nonfamily_household_income.sum_in_area.quantile(percentile_input).astype(str))        \n",
    "\n",
    "        sum_for_percentile_household_type = household_type.sum_in_area.quantile(percentile_input).astype(str)\n",
    "        sum_for_percentile_household_type = sum_for_percentile_household_type.replace(sum_for_percentile_household_type, \\\n",
    "                            household_type.sum_in_area.quantile(percentile_input).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tables_for_percentile_value():\n",
    "#generating new transposed table with only the two fields needed : variables and sum in area.   \n",
    "\n",
    "    global household_type_transposed, language_transposed, education_transposed, family_household_income_transposed, nonfamily_household_income_transposed,household_type_transposed\n",
    "\n",
    "    language_transposed = language.filter([\"variables\", \"sum_in_area\"]).T\n",
    "    language_transposed.columns = language_transposed.iloc[0]\n",
    "    language_transposed = language_transposed[1:]\n",
    "\n",
    "    education_transposed = education.filter([\"variables\", \"sum_in_area\"]).T\n",
    "    education_transposed.columns = education_transposed.iloc[0]\n",
    "    education_transposed = education_transposed[1:]\n",
    "\n",
    "    household_type_transposed = household_type.filter([\"variables\", \"sum_in_area\"]).T\n",
    "    household_type_transposed.columns = household_type_transposed.iloc[0]\n",
    "    household_type_transposed = household_type_transposed[1:]\n",
    "    \n",
    "    family_household_income_transposed = family_household_income.filter([\"variables\", \"sum_in_area\"]).T\n",
    "    family_household_income_transposed.columns = family_household_income_transposed.iloc[0]\n",
    "    family_household_income_transposed = family_household_income_transposed[1:]    \n",
    "    \n",
    "    nonfamily_household_income_transposed = nonfamily_household_income.filter([\"variables\", \"sum_in_area\"]).T\n",
    "    nonfamily_household_income_transposed.columns = nonfamily_household_income_transposed.iloc[0]\n",
    "    nonfamily_household_income_transposed = nonfamily_household_income_transposed[1:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range_for_each_variable():\n",
    "    \n",
    "    global range_table, range_table_all, ranges, first_range, other_ranges\n",
    "\n",
    "    data.sort_values(by=['variable_group', 'sum_in_area'], ascending=[True, True], inplace=True)\n",
    "    data_sorted = data.reset_index()\n",
    "    \n",
    "    ranges=[]\n",
    "    transposed = [education_transposed, family_household_income_transposed, household_type_transposed, \\\n",
    "            language_transposed, nonfamily_household_income_transposed]\n",
    "    \n",
    "    for df in transposed:\n",
    "        for item, i in enumerate(df.columns):\n",
    "            if item == 0:\n",
    "                first_range = np.arange(df.max()[item]+1).astype(int)\n",
    "                ranges.append([first_range])\n",
    "            else:\n",
    "                other_ranges = np.arange(df.min()[item-1]+1, \\\n",
    "                                       df.max()[item]+1).astype(int)\n",
    "                ranges.append([other_ranges])\n",
    "\n",
    "            range_table = pd.DataFrame(data=ranges, index=None, columns=[\"range_per_variable\"])\n",
    "            range_table = range_table.reset_index(drop=True)\n",
    "\n",
    "    range_table_all = pd.merge(range_table, data_sorted, left_index=True, right_index=True, on=None)\n",
    "    range_table_all[\"range_per_variable\"] = range_table_all[\"range_per_variable\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_info_for_text(): \n",
    "    \n",
    "    global result_df\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=None)\n",
    "    for i in range_table_all['range_per_variable']:\n",
    "        if '\\n' in range_table_all:\n",
    "            range_table_all['range_per_variable'].replace(r'\\s+|\\\\n', ' ', regex=True, inplace=True) \n",
    "\n",
    "    sum_for_percentile_language = int(language.sum_in_area.quantile(percentile_input))\n",
    "    sum_for_percentile_education = education.sum_in_area.quantile(percentile_input)\n",
    "    if (sum_for_percentile_education == 'nan'):\n",
    "        sum_for_percentile_education = 0\n",
    "    if (sum_for_percentile_education != 'nan') & (sum_for_percentile_education == 0):\n",
    "        sum_for_percentile_education = int(sum_for_percentile_education)\n",
    "    sum_for_percentile_family_household_income = int(family_household_income.sum_in_area.quantile(percentile_input))\n",
    "    sum_for_percentile_nonfamily_household_income = int(nonfamily_household_income.sum_in_area.quantile(percentile_input))\n",
    "    sum_for_percentile_household_type = int(household_type.sum_in_area.quantile(percentile_input))\n",
    "    \n",
    "    for item,i in enumerate(range_table_all.index):\n",
    "\n",
    "        if int(sum_for_percentile_language) > 0 :\n",
    "            language_only = range_table_all[(range_table_all[\"variable_group\"].str.contains('Language'))]\n",
    "            result = language_only[language_only[\"range_per_variable\"].str.contains(str(sum_for_percentile_language))]\n",
    "            result_df = result_df.append(result, ignore_index = True)\n",
    "\n",
    "        if int(sum_for_percentile_education > 0) & (sum_for_percentile_education != 'nan') :\n",
    "            education_only = range_table_all[(range_table_all[\"variable_group\"].str.contains('Educational'))]\n",
    "            result = education_only[education_only[\"range_per_variable\"].str.contains(str(sum_for_percentile_education))]\n",
    "            result_df = result_df.append(result, ignore_index = True)\n",
    "\n",
    "        if int(sum_for_percentile_family_household_income) > 0:\n",
    "            family_household_income_only = range_table_all[(range_table_all[\"variable_group\"].str.contains('Family'))]\n",
    "            result = family_household_income_only[family_household_income_only[\"range_per_variable\"].str.contains(str(sum_for_percentile_family_household_income))]\n",
    "            result_df = result_df.append(result, ignore_index = True)\n",
    "\n",
    "        if int(sum_for_percentile_nonfamily_household_income) > 0:\n",
    "            nonfamily_household_income_only = range_table_all[(range_table_all[\"variable_group\"].str.contains('Nonfamily'))]\n",
    "            result = nonfamily_household_income_only[nonfamily_household_income_only[\"range_per_variable\"].str.contains(str(sum_for_percentile_nonfamily_household_income))]\n",
    "            result_df = result_df.append(result, ignore_index = True)\n",
    "\n",
    "        if int(sum_for_percentile_household_type) > 0:\n",
    "            household_type_only = range_table_all[(range_table_all[\"variable_group\"].str.contains('Households'))]\n",
    "            result = household_type_only[household_type_only[\"range_per_variable\"].str.contains(str(sum_for_percentile_household_type))]\n",
    "            result_df = result_df.append(result, ignore_index = True)\n",
    "            \n",
    "    result_df = result_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_google_data():\n",
    "    extract_location()\n",
    "\n",
    "    keys_file = open(\"gcs_key.txt\")\n",
    "    APIKEY = keys_file.read().strip()\n",
    "    \n",
    "    global total_results\n",
    "    total_results = []\n",
    "    types = [\"bar\",\"cafe\",\"restaurant\"]\n",
    "\n",
    "    for i in types:\n",
    "        def findPlaces(pagetoken = None):\n",
    "            global lat, lon\n",
    "            lat, lng = (lat,lon)\n",
    "            radius=402\n",
    "            \n",
    "            url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json?location={lat},{lng}&radius={radius}&fields=name,geometry,types,price_level&type={type}&key={APIKEY}{pagetoken}\".format(lat = lat, lng = lng, radius = radius, type = type,APIKEY = APIKEY, pagetoken = \"&pagetoken=\"+pagetoken if pagetoken else \"\")\n",
    "            response = requests.get(url)\n",
    "            res = json.loads(response.text)\n",
    "\n",
    "            for result in res[\"results\"]:\n",
    "                place_name = result['name']\n",
    "                latitude = result[\"geometry\"][\"location\"][\"lat\"]\n",
    "                longitude = result[\"geometry\"][\"location\"][\"lng\"]\n",
    "                place_type = result.get(\"types\",0)\n",
    "                price_level = result.get(\"price_level\",0)\n",
    "                total_results.append([latitude, longitude, place_name,place_type,price_level])\n",
    "\n",
    "            pagetoken = res.get(\"next_page_token\",None)\n",
    "\n",
    "            return pagetoken\n",
    "\n",
    "        pagetoken = None\n",
    "\n",
    "        while True:\n",
    "             pagetoken = findPlaces(pagetoken=pagetoken)\n",
    "             import time\n",
    "             time.sleep(5)\n",
    "\n",
    "             if not pagetoken:\n",
    "                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_google_data_for_text():\n",
    "    import_google_data()\n",
    "    \n",
    "    global places_df, establishment, price_range, list_ranges, very_expensive, expensive, moderately_priced, cheap, num_very_expensive, num_expensive, num_moderately_priced, num_cheap\n",
    "    places_df = pd.DataFrame(data=total_results, columns =[\"latitude\", \"longitude\", \"place_name\",\"place_type\",\"price_level\"]) \n",
    "    \n",
    "    very_expensive = places_df[((places_df[\"price_level\"]).astype(str).str.contains('4'))]\n",
    "    expensive = places_df[((places_df[\"price_level\"]).astype(str).str.contains('3'))]\n",
    "    moderately_priced = places_df[((places_df[\"price_level\"]).astype(str).str.contains('2'))]\n",
    "    cheap = places_df[((places_df[\"price_level\"]).astype(str).str.contains('1'))]\n",
    "    \n",
    "    num_very_expensive = len(very_expensive)\n",
    "    num_expensive = len(expensive)\n",
    "    num_moderately_priced = len(moderately_priced)\n",
    "    num_cheap = len(cheap)\n",
    "    \n",
    "    list_ranges = [num_very_expensive, num_expensive, num_moderately_priced, num_cheap]\n",
    "    \n",
    "    for i in places_df['price_level']:\n",
    "        if max(list_ranges) == num_very_expensive:\n",
    "            price_range = \", are very expensive.\"\n",
    "            establishment = random.choice(very_expensive[\"place_name\"].values) \n",
    "\n",
    "        if max(list_ranges) == num_expensive:\n",
    "            price_range = \", are expensive.\"\n",
    "            establishment = random.choice(expensive[\"place_name\"].values) \n",
    "\n",
    "        if max(list_ranges) == num_moderately_priced:\n",
    "            price_range = \", are moderately expensive.\"\n",
    "            establishment = random.choice(moderately_priced[\"place_name\"].values) \n",
    "\n",
    "        if max(list_ranges) == num_cheap:\n",
    "            price_range = \", are quite cheap.\"\n",
    "            establishment = random.choice(cheap[\"place_name\"].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #AREA TYPE \n",
    "# #% of residents in area \n",
    "# acs_site/...acs population\n",
    "# qwi import ... qwi population\n",
    "1. Import ACS Data\n",
    "2. Import QWI data, import MSAs\n",
    "3. Join ACS to CT, QWI to MSAs\n",
    "4. Proportional Split to get worker and resident populations in Study Area\n",
    "5. Combine Tables\n",
    "6. Calculate % workers, % residents\n",
    "7. Subtract \n",
    "# def get_worker_and_resident_populations():\n",
    "    \n",
    "# get_worker_and_resident_populations()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_narrative():\n",
    "    \n",
    "    clean_combine_census_and_geographic_data()\n",
    "\n",
    "    selected_persona.observe(selected_persona_eventhandler, names='value')\n",
    "    selected_percentile.observe(selected_percentile_eventhandler, names='value')\n",
    "    \n",
    "    get_demographics_for_selection()\n",
    "    parse_tables_for_percentile_value()\n",
    "    get_range_for_each_variable()\n",
    "    generate_info_for_text()\n",
    "    generate_google_data_for_text()\n",
    "    \n",
    "    global result_df, resident_text, \\\n",
    "        intro_text, percentile_text, household_descriptor, establishments_text, affordability_text, \\\n",
    "        household_type_text, language_text, language_adjective, education_text,\\\n",
    "        income_text, income_range, \\\n",
    "        establishment, price_range, price_experience, price_marker, affordability_descriptor, comfort_descriptor,\\\n",
    "        area_type, persona_type\n",
    "    \n",
    "    greetings=[\"Hi, \", \"Hey, \", \"Hello, \"]\n",
    "\n",
    "    persona_type = selected_persona.value.lower()\n",
    "    \n",
    "    #Sort out\n",
    "    area_type = \"predominantly residential area. \"\n",
    "    area_type = \"predominantly commercial area. \"\n",
    "    area_type = \"area that is both commercial and residential. \"\n",
    "\n",
    "    for i in result_df['variable_name']:\n",
    "        if 'Less than high school graduate' in i:\n",
    "            education_text = \" I never got my high school diploma.\"\n",
    "        if 'High school graduate (includes equivalency)' in i:\n",
    "            education_text = 'I am a high school graduate. '\n",
    "        if 'Some college' in i:\n",
    "            education_text = \" While I've attended some form of college, I never completed my degree.\"\n",
    "        if 'Bachelor' in i:\n",
    "            education_text = \" I have a Bachelor's degree, and might even have attained professional or other advanced degrees.\"\n",
    "        else:\n",
    "            education_text = ''\n",
    "\n",
    "        if 'English' in i:\n",
    "            language_text = \"where English is the only language spoken. \"\n",
    "            language_adjective = \"\" \n",
    "        if 'Spanish' in i:\n",
    "            language_text = \"where we speak both English and Spanish. \"\n",
    "        if 'Indo-European' in i:\n",
    "            language_text = \"where we speak both English and an Indo-European language. \"            \n",
    "        if 'Asian' in i:\n",
    "            language_text = \"where we speak both English and an Asian or Pacific Island language. \"            \n",
    "        if 'Other languages' in i:\n",
    "            language_text = \"where we speak another language in addition to English. \"\n",
    "        else: \n",
    "            language_adjective = \" bilingual \"\n",
    "       \n",
    "    #Removing family income from dataframe if it is a non-family household and vice versa \n",
    "        if 'Nonfamily households' in i:\n",
    "            household_type_text = \" away from my family \"\n",
    "            result_df=result_df.loc[~result_df[\"variable_name\"].str.contains('Family Household Income')]\n",
    "        elif 'Family households' in i:\n",
    "            household_type_text = ' with my family '\n",
    "            result_df=result_df.loc[~result_df[\"variable_name\"].str.contains('Nonfamily Household Income')]\n",
    "        else: \n",
    "            household_type_text=' household '\n",
    "            \n",
    "            \n",
    "        if '10,000' or '14,999' in i:\n",
    "            income_text = \" lower income\"\n",
    "            for i in places_df['price_level']:\n",
    "                if (max(list_ranges) == num_very_expensive) | (max(list_ranges) == num_expensive):\n",
    "                    price_marker = \"prohibitively expensive\"\n",
    "                    affordability_descriptor = \"this is a big issue\"\n",
    "                    comfort_descriptor = \"not\"\n",
    "                elif max(list_ranges) == num_moderately_priced:\n",
    "                    price_marker = \"expensive\"\n",
    "                    affordability_descriptor = \"this is an issue\"\n",
    "                    comfort_descriptor = \"barely\"\n",
    "                elif max(list_ranges) == num_cheap:\n",
    "                    price_marker = \"affordable\"\n",
    "                    affordability_descriptor = \"this is not an issue\"\n",
    "                    comfort_descriptor = \"comfortably\"\n",
    "        if '24,999' or '34,999' or '49,999' in i:\n",
    "            income_text = \" middle income\"\n",
    "            for i in places_df['price_level']:\n",
    "                if max(list_ranges) == num_very_expensive:\n",
    "                    price_marker = \"prohibitively expensive\"\n",
    "                    affordability_descriptor = \"this is a big issue\"\n",
    "                    comfort_descriptor = \"not\"\n",
    "                elif max(list_ranges) == num_expensive:\n",
    "                    price_marker = \"expensive\"\n",
    "                    affordability_descriptor = \"this is an issue\"\n",
    "                    comfort_descriptor = \"barely\"\n",
    "                elif (max(list_ranges) == num_cheap) | (max(list_ranges) == num_moderately_priced):\n",
    "                    price_marker = \"affordable\"\n",
    "                    affordability_descriptor = \"this is not an issue\"\n",
    "                    comfort_descriptor = \"comfortably\"\n",
    "        if '74,999' or '94,999' or '149,999' in i:\n",
    "            income_text = \" wealthy\"\n",
    "            for i in places_df['price_level']:\n",
    "                if max(list_ranges) == num_very_expensive:\n",
    "                    price_marker = \"expensive\"  \n",
    "                    affordability_descriptor = \"this is an issue\"\n",
    "                    comfort_descriptor = \"barely\"\n",
    "                elif (max(list_ranges) == num_cheap) | (max(list_ranges) == num_moderately_priced) | (max(list_ranges) == num_expensive):\n",
    "                    price_marker = \"affordable\"\n",
    "                    affordability_descriptor = \"this is not an issue\"\n",
    "        if '199,999' or '200,000' in i:\n",
    "            income_text = \" very wealthy\"\n",
    "            price_marker = \"affordable\"\n",
    "            affordability_descriptor = \"this is not an issue\"\n",
    "            comfort_descriptor = \"comfortably\"\n",
    "\n",
    "\n",
    "    intro_text = random.choice(greetings) + \"I'm a \" + persona_type + \" living in a \" + area_type\n",
    "    percentile_text = str(selected_percentile.value) + \" of my neighbors are like me. \"\n",
    "    household_descriptor = \"I live \" + household_type_text + \" in a \" + income_text + language_adjective + \" household \" + language_text\n",
    "    establishments_text = \" Most of the eating and drinking establishments in this area, such as \" + establishment + price_range\n",
    "    affordability_text = \" Because I am \" + income_text + \", \" + affordability_descriptor + \" as I can \" + comfort_descriptor + \" afford my neighboorhood's bars, cafes and restaurants.\"\n",
    "\n",
    "    resident_text = intro_text + percentile_text + education_text + household_descriptor + establishments_text + affordability_text\n",
    "\n",
    "    with output:\n",
    "        display(resident_text)\n",
    "\n",
    "construct_narrative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_button = Button(description=\"GENERATE NARRATIVE\",\\\n",
    "                               layout=Layout(width='60%', height='50px', border='solid .5px #000'))\n",
    "text_generation_button.style.button_color = '#EDF9FC'\n",
    "text_generation_button.style.font_weight = 'bold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT GENERATION\n",
    "def text_generation(b):\n",
    "    output.clear_output()\n",
    "    data_output.clear_output()\n",
    "    construct_narrative()   \n",
    "    show_dashboard()\n",
    "    \n",
    "text_generation_button.on_click(text_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695d4d189a7c4d4cb6124af60665dd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(ToggleButtons(description='AGE:', options=('Boy', 'Teenage Boy', 'Young Man', 'Mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748d538a18b744d1855d09dd2d7f68d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[40.7210907, -73.9877836], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_titlâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_dashboard():\n",
    "    output.clear_output()\n",
    "    data_output.clear_output()\n",
    "    \n",
    "    item_layout = widgets.Layout(margin='0 0 10px 0', align_items='stretch')\n",
    "    item_layout_tab = widgets.Layout(margin='0 0 10px 0')\n",
    "    \n",
    "    explore_data = range_table_all\n",
    "    explore_data['sum_in_area'] = explore_data['sum_in_area'].astype(int)\n",
    "    \n",
    "    with output:\n",
    "        display(md(\"> <font size = 3, font color = black> {}\".format(resident_text)))\n",
    "    with data_output:\n",
    "        display(explore_data)\n",
    "    \n",
    "    global tab, input_widgets\n",
    "    input_widgets = widgets.VBox(\n",
    "        [selected_persona, selected_percentile, text_generation_button],\n",
    "        layout=item_layout)\n",
    "    \n",
    "    tab = widgets.Tab([output, data_output],\n",
    "        layout=item_layout_tab)\n",
    "    tab.set_title(0, 'Narrative')\n",
    "    tab.set_title(1, 'Dataset')\n",
    "    \n",
    "    global dashboard\n",
    "    dashboard = widgets.VBox([input_widgets, tab])\n",
    "\n",
    "show_dashboard()\n",
    "\n",
    "\n",
    "display(dashboard)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
